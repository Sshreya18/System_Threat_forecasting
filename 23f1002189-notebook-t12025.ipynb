{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":90791,"databundleVersionId":10592855,"sourceType":"competition"}],"dockerImageVersionId":30839,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Importing the train.csv","metadata":{}},{"cell_type":"markdown","source":"#### Importing Data and basic EDA","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/System-Threat-Forecaster/train.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.shape\ndata.head(10)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Creating a copy of data to use in future","metadata":{}},{"cell_type":"code","source":"df = data.copy()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df2 = data.copy()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Importing the Test.csv\n","metadata":{}},{"cell_type":"code","source":"test = pd.read_csv(\"/kaggle/input/System-Threat-Forecaster/test.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"type(test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Name of all the columns present in the train dataset","metadata":{}},{"cell_type":"code","source":"data.columns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"finding how many columns have null values","metadata":{}},{"cell_type":"code","source":"columns_with_null = data.isnull().sum()  \ncolumns_with_null = columns_with_null[columns_with_null > 0]  \ncolumns_with_null.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"target_columns = 'target'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Numerical columns:","metadata":{}},{"cell_type":"code","source":"numerical_columns = data.select_dtypes(include=['float64', 'int64']) \nnumerical_columns.columns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"categorical_columns = data.select_dtypes(include=['category', 'object']) \ncategorical_columns.columns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(8,5))\ntarget_column = data['target'].value_counts()\nplt.pie(target_column, labels = target_column.index, autopct='%1.1f%%')\nplt.title('Target Distribution')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The distribution of target varible is almost equal making it balanced\n* 1 (which means Malware was detected) is about 50.5% of the total systems\n* 0 (which means Malware was not detected) is about 49.5% of the total systems","metadata":{}},{"cell_type":"markdown","source":"finidng correlation between target column and numerical columns and putting them in ascending order","metadata":{}},{"cell_type":"code","source":"numerical_df = data.select_dtypes(include=['float64', 'int64'])\ncorr_matrix = numerical_df.corr()\nplt.figure(figsize=(20, 15))\nsns.heatmap(corr_matrix, annot=False, cmap='coolwarm', linewidths=0.5)\nplt.title('Correlation Heatmap')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Observing Stats details of numerical columns","metadata":{}},{"cell_type":"code","source":"data.describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Plotting graphs to identity patterns","metadata":{}},{"cell_type":"code","source":"data['NumAntivirusProductsInstalled'].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(8,5))\nantivirusinstalled = pd.crosstab(data['NumAntivirusProductsInstalled'], data['target'], normalize = 'index')*100\nax = antivirusinstalled.plot(kind = 'bar')\nfor container in  ax.containers:\n    ax.bar_label(container, fmt= '%1.1f%%', label_type = 'edge')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- More number of AntiVirus Installed reduces the chances of system threat","metadata":{}},{"cell_type":"code","source":"data['NumAntivirusProductsEnabled'].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(8,5))\nantivirusinstalled = pd.crosstab(data['NumAntivirusProductsEnabled'], data['target'], normalize = 'index')*100\nax = antivirusinstalled.plot(kind = 'bar')\nfor container in  ax.containers:\n    ax.bar_label(container, fmt= '%1.1f%%', label_type = 'edge')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data['IsAlwaysOnAlwaysConnectedCapable'].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(8,5))\nantivirusinstalled = pd.crosstab(data['IsAlwaysOnAlwaysConnectedCapable'], data['target'], normalize = 'index')*100\nax = antivirusinstalled.plot(kind = 'bar')\nfor container in  ax.containers:\n    ax.bar_label(container, fmt= '%1.1f%%', label_type = 'edge')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":" - There is slightly less chance of system threath if system is always connected to a cable","metadata":{}},{"cell_type":"code","source":"data['RealTimeProtectionState'].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data['OSVersion'].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data['OSInstallType'].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Milestone 1","metadata":{}},{"cell_type":"code","source":"data['OSVersion'].unique()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data['NumAntivirusProductsInstalled'].max()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data['IsGamer']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gamer_malware = data[(data['IsGamer']== 1) & (data['target']== 1)]\ncount = len(gamer_malware)\ncount","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"passive_mode = data[data['IsPassiveModeEnabled'] == 1]\npassive_mode['RealTimeProtectionState'].mode()[0]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data[(data['PrimaryDisplayResolutionHorizontal'] == 1366) & (data['PrimaryDisplayResolutionVertical']== 768)].shape[0]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data['TotalPhysicalRAMMB'].describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Dealing with nan values","metadata":{}},{"cell_type":"code","source":"total_null = data.isnull().sum().sum()\ntotal_entries = data.size\nnull_percentage = (total_null / total_entries) * 100\nnull_percentage","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"By checking out the data manually, it is found to have values like UNKNOWN, Unknown and unknown. I replaced these with Nan Values","metadata":{}},{"cell_type":"code","source":"data.replace({'unknown': np.nan, 'Unknown': np.nan, 'UNKNOWN': np.nan}, inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Doing the same for test dataset","metadata":{}},{"cell_type":"code","source":"test.replace({'unknown': np.nan, 'Unknown': np.nan, 'UNKNOWN': np.nan}, inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.isna().sum().sum()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Using simple imputer to impute the nan values","metadata":{}},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\n\nnum_cols = data.select_dtypes(include=['int64', 'float64']).columns\ncat_cols = data.select_dtypes(include=['object', 'category']).columns\n\nnum_imputer = SimpleImputer(strategy='most_frequent')\ndata[num_cols] = num_imputer.fit_transform(data[num_cols])\n\ncat_imputer = SimpleImputer(strategy='most_frequent')\ndata[cat_cols] = cat_imputer.fit_transform(data[cat_cols])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"using the same for test dataset to maintain consistency","metadata":{}},{"cell_type":"code","source":"num_cols2 = test.select_dtypes(include=['int64', 'float64']).columns\ncat_cols2 = test.select_dtypes(include=['object', 'category']).columns\n\nnum_imputer = SimpleImputer(strategy='most_frequent')\ntest[num_cols2] = num_imputer.fit_transform(test[num_cols2])\n\ncat_imputer = SimpleImputer(strategy='most_frequent')\ntest[cat_cols2] = cat_imputer.fit_transform(test[cat_cols2])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.isna().sum().sum()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test.isna().sum().sum()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Milestone 2","metadata":{}},{"cell_type":"code","source":"df_copy = df.copy()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_copy2 = df.copy()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"constant_columns = data.nunique()[data.nunique() == 1]\nprint(constant_columns)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n#selecting categorical columns as a list and getting names of the columns\ncategorical_cols = df.select_dtypes(include=['object']).columns\n\nencoder = LabelEncoder()\n#looping through the list and fit and transform on all cols in categorical cols\nfor col in categorical_cols:\n    df[col] = encoder.fit_transform(df[col])\ncorr_matrix = df.corr()\ncorr_pairs = corr_matrix.abs().unstack()\ncorr_pairs = corr_pairs[corr_pairs < 1].sort_values(ascending=False)\ncorr_pairs.head(1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Create a new dataframe called cat_df which contains only the columns of datatype  'object'. In cat_df, for all the columns which take less than or equal to 10 unique values, use OneHotEncoder(sparse=False, drop='first', handle_unknown='ignore'). What is the new number of columns in the cat_df dataframe?","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\ncat_df = df_copy.select_dtypes(include=['object'])\nlow_cardinality_cols = cat_df.nunique()[cat_df.nunique() <= 10].index\n\nencoder = OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore')\nencoded_cols = encoder.fit_transform(cat_df[low_cardinality_cols])\n\nencoded_df = pd.DataFrame(encoded_cols, columns=encoder.get_feature_names_out(low_cardinality_cols))\n\ncat_df = cat_df.drop(columns=low_cardinality_cols)\ncat_df = pd.concat([cat_df, encoded_df], axis=1)\ncat_df.shape[1]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Create a new dataframe called num_df which contains only the columns of datatype  'int64' and 'float64'. Use MinMaxScaler() on num_df. What is the sum of all the values in num_df?","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\nnum_df = df_copy.select_dtypes(include=['int64', 'float64'])\n\nscaler = MinMaxScaler()\nscaled_num_df = pd.DataFrame(scaler.fit_transform(num_df), columns=num_df.columns)\nsum_values = scaled_num_df.sum().sum()\nprint(sum_values)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.isna().sum().sum()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Use the original data (X,y) for this question. Apply the following steps on X:\n1. Use SimpleImputer to fill missing values with strategy as 'most_frequent'\n2. For all columns of datatype 'object', use OrdinalEncoder()\n3. Apply train_test_split with test_size = 0.2 and random_state = 42\n4. Train SGDClassifier with default parameters and random_state = 42\n\nWhat is the accuracy score received on the test data from the train_test_split?\n","metadata":{}},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.metrics import accuracy_score\n\nimputer = SimpleImputer(strategy='most_frequent')\ndf_copy2 = pd.DataFrame(imputer.fit_transform(df_copy2), columns=df_copy2.columns)\n\ncat_cols = df_copy2.select_dtypes(include=['object']).columns\nencoder = OrdinalEncoder()\ndf_copy2[cat_cols] = encoder.fit_transform(df_copy2[cat_cols])\n\nX = df_copy2.drop(columns=['target']) \ny = df_copy2['target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nsgd_model = SGDClassifier(random_state=42)\nsgd_model.fit(X_train, y_train)\n\ny_pred = sgd_model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Test Accuracy:\", accuracy)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Spliting the Dataset into test and train ","metadata":{}},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX = data.drop(columns=['target']) \ny = data['target'] \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_test.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Encoding and Scaling","metadata":{}},{"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OrdinalEncoder\n\nX_train = X_train.copy()\nX_test = X_test.copy()\n\nnum_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\ncat_cols = X_train.select_dtypes(include=[object, 'category']).columns.tolist()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"preprocessor = ColumnTransformer(\n    transformers=[\n        ('cat', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1), cat_cols)  \n    ],\n    remainder='passthrough' \n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train = preprocessor.fit_transform(X_train)\nX_test = preprocessor.transform(X_test)\ntest = preprocessor.transform(test)\n\nall_columns = cat_cols + num_cols \nX_train = pd.DataFrame(X_train, columns=all_columns)\nX_test = pd.DataFrame(X_test, columns=all_columns)\ntest = pd.DataFrame(test, columns=all_columns)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"scaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\ntest = scaler.transform(test)\n\nX_train = pd.DataFrame(X_train, columns=all_columns)\nX_test = pd.DataFrame(X_test, columns=all_columns)\ntest = pd.DataFrame(test, columns=all_columns)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_train","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Trying out different Models","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nclf = LogisticRegression() \nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy: \", accuracy)\n\nconf_matrix = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix:\\n\", conf_matrix)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\nknn.fit(X_train, y_train)\n\ny_pred = knn.predict(X_test)\n\naccuracy1 = accuracy_score(y_test, y_pred)\nprint(\"Accuracy: \", accuracy1)\n\nconf_matrix = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix:\\n\", conf_matrix)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n\nrf_classifier.fit(X_train, y_train)\n\ny_pred = rf_classifier.predict(X_test)\n\naccuracy2 = accuracy_score(y_test, y_pred)\n\nprint(\"Accuracy:\", accuracy2)\n\nconf_matrix = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix:\\n\", conf_matrix)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import xgboost as xgb\n\ndtrain = xgb.DMatrix(X_train, y_train)\ndtest = xgb.DMatrix(X_test)\n\nparams = {\n    'objective': 'binary:logistic',  \n    'eval_metric': 'logloss',  \n    'max_depth': 6,\n    'learning_rate': 0.1,\n    'n_estimators': 100,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'random_state': 42\n}\n\nmodel = xgb.XGBClassifier(**params)\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\naccuracy3 = accuracy_score(y_test, y_pred)\nprint(\"Accuracy Score:\", accuracy3)\n\nconf_matrix = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix:\\n\", conf_matrix)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import lightgbm as lgb\n\ntrain_data = lgb.Dataset(X_train, y_train)\ntest_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n\nparams = {\n    'objective': 'binary',  \n    'metric': 'binary_logloss',  \n    'boosting_type': 'gbdt', \n    'max_depth': -1,  \n    'learning_rate': 0.1,\n    'n_estimators': 100,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'random_state': 42\n}\n\nmodel2 = lgb.LGBMClassifier(**params)\nmodel2.fit(X_train, y_train)\n\ny_pred = model2.predict(X_test)\n\naccuracy4 = accuracy_score(y_test, y_pred)\nprint(\"Accuracy Score: \", accuracy4)\n\nconf_matrix = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix:\\n\", conf_matrix)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_accuracies = {\n    \"Logistic Regression\": accuracy,\n    \"KNN\" : accuracy1,\n    \"Random Forest\": accuracy2,\n    \"XGBClassifier\": accuracy3,\n    \"LGBMClassifier\": accuracy4\n}\n\nmodel_accuracies","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"acc_df = pd.DataFrame(list(model_accuracies.items()), columns=[\"Model\", \"Accuracy\"])\nacc_df = acc_df.sort_values(by=\"Accuracy\", ascending=False)  \nprint(acc_df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\n\nplt.figure(figsize=(8, 5))\nsns.barplot(x=list(model_accuracies.keys()), y=list(model_accuracies.values()), palette=\"viridis\")\nplt.xlabel(\"Models\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"Model Accuracy Comparison\")\nplt.ylim(0, 1) \nplt.xticks(rotation=45)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Hyper-Parameter Tuning Using RandomizedSearchCV","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\nrf= RandomForestClassifier()\nparam_dist = {\n    'n_estimators': [50, 100, 200, 300],\n    'max_depth': [None, 10, 20, 30], \n    'min_samples_split': [2, 5, 10],  \n}\n\nrandom_search = RandomizedSearchCV(\n    rf, param_distributions=param_dist, \n    n_iter=10, scoring='accuracy', \n    cv=3, verbose=1, random_state=42, n_jobs=-1\n)\n\nrandom_search.fit(X_train, y_train)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"random_search.best_params_","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rf_tuned = random_search.best_estimator_\ny_pred = rf_tuned.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\naccuracy","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"xgb_tuned = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\nparam_dist = {\n    'n_estimators': [50, 100, 200],  \n    'max_depth': [3, 6, 9], \n    'learning_rate': [0.01, 0.05, 0.1] \n}\n\nrandom_search = RandomizedSearchCV(\n    xgb_tuned, param_distributions=param_dist, \n    n_iter=100, scoring='accuracy', \n    cv=3, verbose=1, random_state=42, n_jobs=-1\n)\n\n\nrandom_search.fit(X_train, y_train)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"xgb_tuned = random_search.best_estimator_\ny_pred = xgb_tuned.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\n\nrandom_search.best_params_\naccuracy","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lgbm_model = lgb.LGBMClassifier(objective='binary', metric='binary_logloss', random_state=42)\n\nparam_dist = {\n    'n_estimators': [50, 100, 200], \n    'max_depth': [-1, 5, 10], \n    'learning_rate': [0.01, 0.05, 0.1] \n}\n\nrandom_search = RandomizedSearchCV(\n    lgbm_model, param_distributions=param_dist, \n    n_iter=100, scoring='accuracy', \n    cv=3, verbose=1, random_state=42, n_jobs=-1\n)\n\nrandom_search.fit(X_train, y_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lgbm_tuned = random_search.best_estimator_\ny_pred = lgbm_tuned.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\naccuracy","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"random_search.best_params_","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Milestone 3","metadata":{}},{"cell_type":"markdown","source":"Fit a Decision tree model (random state 42) on the training set and perform hyper parameter tuning using grid search with 3 folds and use scoring as accuracy, using the following values:\n\nmax_depth: [20, 30]\nmin_samples_split: [2, 5]\nmin_samples_leaf: [1, 2]","metadata":{}},{"cell_type":"markdown","source":"Trying GRIDSEARCH For DecisionTreeClassifier","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\n\ndt = DecisionTreeClassifier(random_state=42)\n\nparam_grid1 = {\n    'max_depth': [20, 30],\n    'min_samples_split': [2, 5],\n    'min_samples_leaf': [1, 2]\n}\n\ngrid_search = GridSearchCV(\n    estimator=dt,\n    param_grid=param_grid1,\n    cv=3, \n    scoring='accuracy',   \n)\n\ngrid_search.fit(X_train, y_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_params = grid_search.best_params_\nbest_params","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"clf = DecisionTreeClassifier(**best_params)  \nclf.fit(X_train, y_train) \n\naccuracy = clf.score(X_test, y_test)\naccuracy","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Fit an AdaBoostClassifier model (random state 42) on the training set and perform hyper parameter tuning using grid search with 3 folds and use scoring as accuracy, using the following values :\n\na. n_estimators: [10, 20, 30]\n\nb. learning_rate: [5, 10]\n\nc. algorithm: ['SAMME']","metadata":{}},{"cell_type":"markdown","source":"Trying GRIDSEARCH For ADABOOSTCLASSIFIER","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\nadaboost = AdaBoostClassifier( random_state=42)\nparam_grid2 = {\n    'n_estimators': [10, 20, 30],    \n    'learning_rate': [5, 10],        \n    'algorithm': ['SAMME']           \n}\n\ngrid_search = GridSearchCV(\n    estimator=adaboost,\n    param_grid=param_grid2,\n    scoring='accuracy',  \n    cv=3,  \n)\n\ngrid_search.fit(X_train, y_train)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_params2 = grid_search.best_params_\nbest_params2","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ada = AdaBoostClassifier(**best_params2)  \nada.fit(X_train, y_train) \n\naccuracy = ada.score(X_test, y_test)\naccuracy","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Experimenting with PCA","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import PCA\npca = PCA()\nX_train_pca = pca.fit_transform(X_train)  \nX_test_pca = pca.transform(X_test) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(8, 5))\nplt.plot(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_, marker='o', linestyle='--')\nplt.xlabel('Principal Component')\nplt.ylabel('Variance Explained')\nplt.title('Scree Plot')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"1. Elbow Point for Optimal Components\n   - The variance drops steeply at first, then levels off.  \n   - The point where the slope changes suggests that keeping 10-15 components is sufficient to capture most of the variance.  \n\n\n2. Diminishing Returns \n   - The first few components explain most variance.  \n   - After 20 components, adding more doesn't contribute much.  \n\n","metadata":{}},{"cell_type":"code","source":"cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\nplt.figure(figsize=(8, 5))\nplt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='--', color='b')\nplt.xlabel('Number of Components')\nplt.ylabel('Cumulative Explained Variance')\nplt.title('Cumulative Explained Variance Plot')\nplt.axhline(y=0.95, color='r', linestyle='--', label='95% Variance')\nplt.axhline(y=0.90, color='g', linestyle='--', label='90% Variance')\nplt.axhline(y=0.85, color='orange', linestyle='--', label='85% Variance')\nplt.legend()\nplt.grid(True)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n* for simpler model with good compression, 25-30 PCs (85% variance) is enough.\n* for high accuracy with minimal loss, 30-35 PCs (90% variance) is ideal.\n* for preserve almost all information, 40-45 PCs (95% variance) is necessary \n","metadata":{}},{"cell_type":"code","source":"pca = PCA(n_components = 0.9)\nX_train_pca_90 = pca.fit_transform(X_train)  \nX_test_pca_90 = pca.transform(X_test) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train_pca_90","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"While using PCA with 90% variance retention, Accuracy decreases :(","metadata":{}},{"cell_type":"code","source":"lgbm_tuned.fit(X_train_pca_90, y_train)\n\ny_pred = lgbm_tuned.predict(X_test_pca_90)\n\naccuracy4 = accuracy_score(y_test, y_pred)\naccuracy4","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Using PCA also decreases the accuracy for XGBClassifier","metadata":{}},{"cell_type":"code","source":"xgb_tuned.fit(X_train_pca_90, y_train)\ny_pred = xgb_tuned.predict(X_test_pca_90)\naccuracy2 = accuracy_score(y_test, y_pred)\naccuracy2","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Milestone 4","metadata":{}},{"cell_type":"markdown","source":"The preprocessing steps mentioned below are to be used for all the questions that are a part of this milestone\nDrop the rows with missing values from your training data\nSeparate the features(X) and the target vector(y)\nEncode the columns of type 'object' or 'category' using ordinal encoder\nScale the entire dataset using StandardScaler","metadata":{}},{"cell_type":"code","source":"df2.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nle = LabelEncoder()\n\ncat_cols = df2.select_dtypes(include=['object']).columns\nnum_cols = df2.select_dtypes(include=['int32', 'int64', 'float64']).columns\n\n#imputing\nmean_imp = SimpleImputer(strategy='mean')  \nmode_imp = SimpleImputer(strategy='most_frequent') \n\ndf2[num_cols] = mean_imp.fit_transform(df2[num_cols]) \ndf2[cat_cols] = mode_imp.fit_transform(df2[cat_cols]) \n\n#label encoding on categorical columns\nfor col in cat_cols:\n    df2[col] = le.fit_transform(df2[col]) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_cols = df2.select_dtypes(include=['int32', 'int64', 'float64']).columns.drop('target', errors='ignore')\nscaler = StandardScaler()\ndf2[num_cols] = scaler.fit_transform(df2[num_cols])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df2.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X = df2.drop(columns=['target'])  \ny = df2['target']\nX_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pca2 = PCA()  \nX_pca = pca2.fit_transform(df2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"type(X_pca)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"What is the number of components required to explain 70% of the variance?","metadata":{}},{"cell_type":"code","source":"cumulative_variance = np.cumsum(pca2.explained_variance_ratio_)\nn_70 = np.argmax(cumulative_variance >= 0.70) + 1\nn_70","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cumulative_variance.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Reconstruct the data using the number of components obtained in the previous question and enter the Mean Squared Error value obtained","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\npca_70 = PCA(n_components=n_70)\nX_reduced = pca_70.fit_transform(df2)\nX_reconstructed = pca_70.inverse_transform(X_reduced)\nmse = mean_squared_error(df2, X_reconstructed)\nmse","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"How much variance is explained by 40 components?","metadata":{}},{"cell_type":"code","source":"cumulative_variance = np.cumsum(pca2.explained_variance_ratio_)\nexplained_variance_40 = cumulative_variance[39]\nexplained_variance_40","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Select top 15 features from the train dataset using f_classif as the score function","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import VarianceThreshold\nfrom sklearn.feature_selection import SelectKBest, f_classif\n\nconstant_filter = VarianceThreshold(threshold=0)  # Remove features with zero variance\nX_train_filtered = constant_filter.fit_transform(X_train)\nfiltered_features = X_train.columns[constant_filter.get_support()]\nfiltered_features   #retained columns after remvoing","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"selector = SelectKBest(k=15)\nX_train_selected = selector.fit_transform(X_train_filtered, y_train)\nselected_features = filtered_features[selector.get_support()]\nselected_features","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"What is the score of the best feature as obtained?","metadata":{}},{"cell_type":"code","source":"f_scores = selector.scores_\nbest_feature_score = max(f_scores)\nbest_feature_score","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_feature_index = f_scores.argmax()\nbest_feature = filtered_features[best_feature_index]\nbest_feature","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Make use of different feature selection methods and describe your process and findings. \nFit Lasso and Ridge models and provide your insights on the values of the coefficients obtained","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\n\nmodel = LinearRegression()\n\nrfe = RFE(estimator=model, n_features_to_select=15)\nX_train_selected = rfe.fit_transform(X_train, y_train)\nX_test_selected = rfe.transform(X_test)\nselected_features = X_train.columns[rfe.support_]\nselected_features","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.linear_model import Lasso\n\nlasso = Lasso(alpha=0.1)\nlasso.fit(X_train_selected, y_train)\n\ny_pred_lasso = lasso.predict(X_test_selected)\n\nmse_lasso = mean_squared_error(y_test, y_pred_lasso)\nmse_lasso","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lasso_coeffs = pd.Series(lasso.coef_, index=selected_features)\nlasso_coeffs.sort_values(ascending = False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.linear_model import Ridge\n\nridge = Ridge(alpha=0.1)\nridge.fit(X_train_selected, y_train)\ny_pred_ridge = ridge.predict(X_test_selected)\nmse_ridge = mean_squared_error(y_test, y_pred_ridge)\nmse_ridge","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ridge_coeffs = pd.Series(ridge.coef_, index=selected_features)\nridge_coeffs.sort_values(ascending = False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred2 = model2.predict(test)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred2.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission = pd.DataFrame({\"id\" : range(0,test.shape[0]),\n                          \"target\" : y_pred2})\nsubmission.to_csv('submission.csv' ,index = False)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}